{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/salah/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/salah/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import seq2seq\n",
    "from tensorflow.contrib.rnn import *\n",
    "from tensorflow.python.layers import core\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/salah/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# ...  \n",
    "#filtered_words = list(filter(lambda word: word not in stopwords.words('english'), word_list))\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How many calories are in a taco from taco villa ?\\n', 'What is italy s primary exports ?\\n']\n",
      "300000\n",
      "300000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['How can you shop online without a credit card in canada ',\n",
       " 'Most popular X men ',\n",
       " 'Akbar s 9 ratna ',\n",
       " 'Where is oil filter on 1994 protoge ',\n",
       " 'How do you beat the pink thing in pirate islant in poptropica ',\n",
       " 'What is the french name for dollar bills ',\n",
       " 'What are Molecules that are always moving randomly ',\n",
       " 'Are dolphins carnivore or ominvore ',\n",
       " 'Where was an apple found ',\n",
       " 'How many proteins in chick peas ']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../src_medium_train.txt\", encoding=\"utf8\") as f:\n",
    "    src_questions = f.readlines()\n",
    "print(src_questions[-3:-1])\n",
    "src_questions = [i[0:-2] for i in src_questions]\n",
    "with open(\"../tgt_medium_train.txt\",encoding=\"utf8\" ) as f:\n",
    "    tgt_questions = f.readlines()\n",
    "tgt_questions = [i[0:-2] for i in tgt_questions]\n",
    "all_questions  = src_questions + tgt_questions\n",
    "print(len(tgt_questions))\n",
    "print(len(src_questions))\n",
    "src_questions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([26492, 6, 30, 211], 4)\n",
      "CPU times: user 5.15 s, sys: 42 ms, total: 5.19 s\n",
      "Wall time: 7.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "import pprint\n",
    "from collections import Counter\n",
    "enc_sentence_length = 18\n",
    "dec_sentence_length = 18\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    tokens = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence)\n",
    "    return tokens\n",
    "\n",
    "_START_ = \"_GO_\"\n",
    "_PAD_ = \"_PAD_\"\n",
    "_END_ = \"_END_\"\n",
    "_UNK_ = \"_UNK_\"\n",
    "\n",
    "def build_vocab(sentences, max_vocab_size=None):\n",
    "    word_counter = Counter()\n",
    "    vocab = dict()\n",
    "    reverse_vocab = dict()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence)\n",
    "        word_counter.update(tokens)\n",
    "        \n",
    "    if max_vocab_size is None:\n",
    "        max_vocab_size = len(word_counter)\n",
    "    #vocab[_UNK_] = -1\n",
    "    vocab[_START_] = 0\n",
    "    vocab[_PAD_] = 1\n",
    "    vocab[_END_] = 2\n",
    "    vocab_idx = 3\n",
    "    for key, value in word_counter.most_common(max_vocab_size):\n",
    "            vocab[key] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "            \n",
    "    for key, value in vocab.items():\n",
    "        reverse_vocab[value] = key\n",
    "            \n",
    "    return vocab, reverse_vocab, max_vocab_size\n",
    "\n",
    "vocab,reverse_vocab,max_vocab_size = build_vocab(all_questions)\n",
    "\n",
    "\n",
    "def token2idx(word, vocab, training=True):\n",
    "    if training :\n",
    "        return vocab[word]\n",
    "    else : \n",
    "        if word in vocab.keys():\n",
    "            return vocab[word]\n",
    "        else : \n",
    "            return 1\n",
    "\n",
    "def sent2idx(sent, vocab=vocab, max_sentence_length=enc_sentence_length, is_target=False, training= True):\n",
    "    tokens = tokenizer(sent)\n",
    "        tokens = list(filter(lambda word: word not in stopwords.words('english'), tokens))\n",
    "\n",
    "    current_length = len(tokens)\n",
    "    pad_length = max_sentence_length - current_length\n",
    "    if current_length >max_sentence_length : \n",
    "        real_tokens = tokens [0:max_sentence_length]\n",
    "        return [token2idx(token, vocab, training) for token in real_tokens] , max_sentence_length\n",
    "    if is_target:\n",
    "        return [0] + [token2idx(token, vocab, training) for token in tokens] + [2] , current_length + 1\n",
    "    else:\n",
    "        return [token2idx(token, vocab, training) for token in tokens] , current_length\n",
    "\n",
    "def idx2token(idx, reverse_vocab):\n",
    "    return reverse_vocab[idx]\n",
    "\n",
    "def idx2sent(indices, reverse_vocab=reverse_vocab):\n",
    "    return \" \".join([idx2token(idx, reverse_vocab) for idx in indices])\n",
    "\n",
    "len(vocab)\n",
    "print(sent2idx('Hi What is your name?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [08:31<00:00, 586.67it/s]\n"
     ]
    }
   ],
   "source": [
    "sent_idx = []\n",
    "for i in tqdm.tqdm(range(len(src_questions))):\n",
    "    sent_idx.append( sent2idx(src_questions[i], max_sentence_length=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "word1= \"world\"\n",
    "word2=\"politics\"\n",
    "token1= token2idx(word1, vocab)\n",
    "token2= token2idx(word2, vocab)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([12, 1748, 339, 420, 1415, 300, 342], 7),\n",
       " ([1539, 244, 4034, 832], 4),\n",
       " ([33814, 741, 48252], 3),\n",
       " ([26, 161, 186, 563, 48253], 5),\n",
       " ([12, 379, 1947, 252, 4384, 48254, 791], 7),\n",
       " ([6, 463, 30, 146, 7840], 5),\n",
       " ([6, 21611, 2067, 5601, 22634], 5),\n",
       " ([56, 3373, 2284, 43566], 4),\n",
       " ([26, 837, 182], 3),\n",
       " ([12, 16, 2931, 7022, 9398], 5)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_idx[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_common_words(word) : \n",
    "    final_dict=dict() \n",
    "    token1= token2idx(word, vocab)\n",
    "    for i in tqdm.tqdm(range(len(sent_idx))) :\n",
    "        sent = sent_idx[i][0]\n",
    "        if token1 in sent : \n",
    "            for j in sent :\n",
    "                if j in final_dict.keys():\n",
    "                    final_dict[j]+=1\n",
    "                else : \n",
    "                    final_dict[j]= 1 \n",
    "    return final_dict\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n",
    "sorted_x = sorted(x.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 1169817.87it/s]\n"
     ]
    }
   ],
   "source": [
    "stats = get_common_words(\"football\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "\n",
    "\n",
    "def get_n_longest_values_g(dictionary, n):\n",
    "    longest_entries = sorted(dictionary.items(), key=takeSecond,reverse=True)[:n]\n",
    "    return [longest_entrie[0] for longest_entrie in longest_entries]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[378,\n",
       " 6,\n",
       " 12,\n",
       " 16,\n",
       " 22,\n",
       " 303,\n",
       " 427,\n",
       " 1679,\n",
       " 694,\n",
       " 41,\n",
       " 275,\n",
       " 1119,\n",
       " 51,\n",
       " 67,\n",
       " 217,\n",
       " 152,\n",
       " 227,\n",
       " 26,\n",
       " 930,\n",
       " 2076,\n",
       " 633,\n",
       " 118,\n",
       " 50,\n",
       " 34,\n",
       " 1600,\n",
       " 33,\n",
       " 45,\n",
       " 4466,\n",
       " 79,\n",
       " 30,\n",
       " 86,\n",
       " 65,\n",
       " 1837,\n",
       " 1264,\n",
       " 85,\n",
       " 709,\n",
       " 244,\n",
       " 8560,\n",
       " 399,\n",
       " 54,\n",
       " 31,\n",
       " 168,\n",
       " 1163,\n",
       " 59,\n",
       " 1916,\n",
       " 52,\n",
       " 590,\n",
       " 1539,\n",
       " 122,\n",
       " 74]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_30 =get_n_longest_values_g(stats,50)\n",
    "stats_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['football',\n",
       " 'What',\n",
       " 'How',\n",
       " 'many',\n",
       " 'much',\n",
       " 'team',\n",
       " 'player',\n",
       " 'players',\n",
       " 'field',\n",
       " 'Who',\n",
       " 'american',\n",
       " 'professional',\n",
       " 'When',\n",
       " 'Which',\n",
       " 'college',\n",
       " 'invented',\n",
       " 'play',\n",
       " 'Where',\n",
       " 'manager',\n",
       " 'teams',\n",
       " 'club',\n",
       " 'size',\n",
       " 'people',\n",
       " '-',\n",
       " 'rules',\n",
       " 'make',\n",
       " 'Why',\n",
       " 'helmet',\n",
       " 'become',\n",
       " 'name',\n",
       " 'worth',\n",
       " 'best',\n",
       " 'width',\n",
       " 'score',\n",
       " 'country',\n",
       " 'position',\n",
       " 'popular',\n",
       " 'regulations',\n",
       " 'game',\n",
       " 'year',\n",
       " 'get',\n",
       " 'earn',\n",
       " 'pro',\n",
       " 'The',\n",
       " 'league',\n",
       " 'Is',\n",
       " 'basketball',\n",
       " 'Most',\n",
       " 'big',\n",
       " 'world']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2token(i, reverse_vocab) for i in stats_30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 1526250.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[259,\n",
       " 6,\n",
       " 244,\n",
       " 179,\n",
       " 12,\n",
       " 227,\n",
       " 256,\n",
       " 67,\n",
       " 26,\n",
       " 224,\n",
       " 52,\n",
       " 1029,\n",
       " 114,\n",
       " 79,\n",
       " 2076,\n",
       " 1643,\n",
       " 1536,\n",
       " 51,\n",
       " 1539,\n",
       " 45,\n",
       " 2336,\n",
       " 139,\n",
       " 9204,\n",
       " 247,\n",
       " 1960,\n",
       " 3817,\n",
       " 334,\n",
       " 54,\n",
       " 61,\n",
       " 16,\n",
       " 59,\n",
       " 1096,\n",
       " 416,\n",
       " 3933,\n",
       " 74,\n",
       " 809,\n",
       " 65,\n",
       " 4973,\n",
       " 34,\n",
       " 89,\n",
       " 3752,\n",
       " 492,\n",
       " 2898,\n",
       " 277,\n",
       " 205,\n",
       " 549,\n",
       " 346,\n",
       " 1119,\n",
       " 391,\n",
       " 470]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politics = get_common_words(\"sport\")\n",
    "politics_30 = get_n_longest_values_g(politics,50)\n",
    "politics_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_words(list1, list2) : \n",
    "    inter = set(list1).intersection(list2)\n",
    "    return inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_try = common_words(stats_30, politics_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best',\n",
       " '-',\n",
       " 'Which',\n",
       " 'play',\n",
       " 'Most',\n",
       " 'What',\n",
       " 'world',\n",
       " 'How',\n",
       " 'Why',\n",
       " 'become',\n",
       " 'many',\n",
       " 'When',\n",
       " 'popular',\n",
       " 'Is',\n",
       " 'year',\n",
       " 'Where',\n",
       " 'The',\n",
       " 'teams',\n",
       " 'professional']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_common = [idx2token(i, reverse_vocab) for i in first_try]\n",
    "get_common"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
